\documentclass{article}
\usepackage{listings}
\usepackage{graphicx}

\lstset{ %
language=Java,                  % the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it's 1, each line 
                                % will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,                   % adds a frame around the code
tabsize=2,                      % sets default tabsize to 2 spaces
captionpos=t,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
title=\lstname,                 % show the filename of files included with \lstinputlisting;
}

\usepackage[parfill]{parskip}
\begin{document}

\title{Forward and backward-chaining for Testing Satisfiability of Propositional Horn Clauses \protect\\ Midterm Report}
\author{Luke Slater}

\maketitle

\section{Introduction}

The problem I propose to investigate is one of testing entailment and
satisfiability of given statements together with a knowledgebase, for a particular 
type of logical structure. Issues of 
logical knowledge representation and the possibility of automatically reasoning
with them have been considered for a long time in the fields of Computer Science
and Mathematics, and many unsolved problems
remain in the field. Even for first-order logic it is not possible to create a
decidable algorithm to prove that any given statement is entailed.

As such, this work will use one particular subset of first-order logic
represented in Horn clauses, which allows for relatively easy and
decidable testing of entailment. The two approaches considered seem
superficially quite similar, but are actually vastly different in terms of
time complexity and decidability.

\section{Description of Problem}

The problem concerns determining the satisfiability of a knowledge base of Horn
clauses when combined with a set of input literals. That is, it decides whether
a knowledgebase logically entails (can model) a given set of inputs.

Horn clauses are a data structure which represent simple conditional 'if; then'
sentences, sometimes called 'rules,' which form a fragment of First Order Logic.

They work as a set of literally logically ORed together, with at most one
positive literal. In the positive Horn clause, which is one with exactly one
positive literal, this forms an implication. For example, the Horn clause 
[ $\neg$Child, $\neg$Female, Girl ] is equivalent to the sentence "Either a
Child, or a Female, or a Girl." which is in turn logically equivalent to the
more intuitive First Order Logic statement 
"$\neg$Child $\vee$ $\neg$Female $\Rightarrow$ Girl."

In this investigation, only propositional Horn clauses will be considered -
which means that we exclude the use of quantifiers and variables. This is to
simplify automated reasoning, in fact propositional Horn clauses are considered
particularly in the field of logic because as a fragment of First Order Logic 
their joint satisfiability is determinable through resolution.

The process of resolution when concerned with propositional Horn clauses is
called refutation-complete\cite{refcomp}, which means that if there is a
contradiction in a set of clauses, there is a path to that contradiction through
the process of resolution.\cite{resolution}

This means, on the basis of the inverse satisfiability problem\cite{invsat},
that resolution can be used to determine the satisfiability of a
literal given a knowledgebase - rather, whether the knowledgebase entails the
given literal.

This is possible because the assertion KB $\vdash$ T is logically equivalent to
KB $\cup$ $\neg$T $\vdash$ $\bot$. That is, one can prove that the knowledgebase
entails a given input literal by deriving a contradiction from the negation of
the literal with the knowledgebase.

However, just because there is a potential route to the contradiction possible,
doesn't necessarily mean that it will be found by a particular implementation of
resolution. In terms of implementation there are two primary methods, 
forward-chaining and backward-chaining.

\section{Algorithms}

Both algorithms operate on two inputs:

\begin{enumerate}
  \item A set of positive Horn clauses $K = k_1 .. k_n$, for which each Horn
  clause is a list formed of $a_2 .. a_l$ negative literals and the positive 
  literal $a_1$. 
  \item A list of literals $P = p_1 .. p_m$.
\end{enumerate}

Using these inputs, the algorithms return $TRUE$ if a contradiction can be derived
from the knowledgebase together with the negated forms of the literals $P$, thus proving
$K \vdash P$, and $FALSE$ otherwise.

\subsection{Forward Chaining}

Forward chaining instead works by starting with the facts in the knowledgebase
and works towards the goals given as input. It marks each atomic constituent of
the input as 'solved' once it finds they are entailed by the knowledgebase.

There are many variations of the forward-chaining approach to Horn clause
satisfiability, but the one considered here will be a polynomial-time approach.
The naive implementation simply 

\subsubsection{Description}

\subsubsection{Correctness}

\subsubsection{Time Complexity}

\subsection{Backward Chaining}

The backward chaining algorithm is so-called because it works backwards, from the
'goals' towards facts in the database. It is a recursive depth-first search, since 
it solves the sub-goals turned up by each input before progressing to the next 
item in the input.

\subsubsection{Description}

This algorithm is defined by a simple function which is called initially with
our set of goals from input $P$. We then iterate our knowledgebase $K$ looking for a
clause $C$ with a positive literal $c_1$ which matches with our first negative
literal $p_1$. If one is found, we remove $p_1$ from $P$, and instead prepend
the negative literals $c_2 .. c_h$ from our matching clause $C$.

After having done this, we call solve recursively with the modified $P$, until
the list of goals $P$ is empty, upon which we return $TRUE$. If we're not able
to find a matching positive literal in the knowledgebase for our $p_1$, the
function will return $FALSE$ - if we're below the top level we will then return
to the calling function, which will continue to search for another clause $C$ to
match its $p_1$; this property forms the depth-first search.

\begin{lstlisting}[mathescape=true]
def solve[$p_1 .. p_m$]
  if m = 0 return $TRUE$

  for each clause $C$ with length $h$ in $K$
    if $c_1$ = $p_1$ and solve[ $c_2 .. c_h, p_2 .. p_m$ ]
      return $TRUE$
  
  return $FALSE$
\end{lstlisting}

\subsubsection{Correctness}

This algorithm will always return the correct answer only if it finishes, which
makes it semi-decidable. It is possible to cause an infinite loop by resolving
with tautologous statements. Consider, for example, a knowledgebase $K$ constituting
the clause [ $\neg$Girl, Girl ] and the negated literal [ $\neg$Girl ] for our
input $P$. In this case, we will resolve the input with our one knowledgebase 
clause to gain the resolvent $\neg$Girl, which will again resolve with the
knowledgebase clause infinitely. Because of this, it is also not safe to say
that it will finish if there is an answer to be found, because we may fall into
an infinite loop before matching our input with a later knowledgebase clause
which would lead to a satsifactory proof.

However, in most cases it will return the correct answer; despite this flaw
backwards-chaining is used for many practical applications including in the
logic programming language Prolog.

\subsubsection{Time Complexity}

The worst-case for this algorithm will always be infinite, since the algorithm
is undecidable. However, we can still consider the average case.

If we consider the case in which we select an arbitrary $n$, and generate a grammar 
with $2n$ literals: $p_0 .. p_{n-1}, q_0 .. q_{n-1}$ and include $4n-4$ clauses in the knowledgebase
generated like so:

\begin{lstlisting}[mathescape=true]
for i..n
  $p_{i-1}$ $\Rightarrow$ $p_i$
  $q_{i-1}$ $\Rightarrow$ $p_i$
  $p_{i-1}$ $\Rightarrow$ $q_i$
  $q_{i-1}$ $\Rightarrow$ $q_i$
\end{lstlisting}

If we then select a random literal from our grammar, 

To solve $p_i$ we will need $2^i$ steps to solve, meaning the worst case (if it
finishes) is $2^n$, or exponential.

\subsection{Discussion}


\begin{thebibliography}{9}

\bibitem{krr}
  Ronald J. Brachman, Hector J. Levesque.
  \emph{Knowledge Representation \& Reasoning}.
  2004.

\bibitem{horn}
  Alfred Horn.
  \emph{On sentences which are true of direct unions of algebras}
  Journal of Symbolic Logic, 16:14-21
  1951.
\end{thebibliography}

\end{document}
